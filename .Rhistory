t=seq(from=0,to=8, by=0.01)
out<-ode(y=v_ini, times=t, func=derivs7, parms=c(a=2, b=4, c=3))
plot(t, out[,2], type="l")
manual_soln<- function(t) exp(-t)*(1+ (1/0.707)*sin(0.707*t))
y<- manual_soln(t)
plot(t, y, type="l")
par(new(T))
par(new=T)
plot(t, out[,2], type="l")
plot(t, out[,2], type="l", col="purple")
par(new=T)
plot(t, y, type="l")
dev.off()
par(new=T)
t=seq(from=0,to=8, by=0.01)
out<-ode(y=v_ini, times=t, func=derivs7, parms=c(a=2, b=4, c=3))
plot(t, out[,2], type="l", col="purple")
manual_soln<- function(t) exp(-t)*(1+ (1/0.707)*sin(0.707*t))
y<- manual_soln(t)
plot(t, y, type="l")
fun_datasets<- function(m, sample_size, trial_size, p) {
dataset<-sapply(seq(1:m), function(t) {rbinom(sample_size, trial_size, p)} )
sample_means<-apply(dataset, 2, mean)
return(sample_means)
}
# Create datasets
meansa<- fun_datasets(10000,20, 10, 0.01)
meansb<- fun_datasets(10000, 100, 10, 0.01)
par(mfrow=c(1,2))
hist(meansa)
hist(meansb)
dev.off()
x<-rnorm(20,100,4)
hist(x)
epsilon<-rnorm(20, 0, 1)
y<- 2+ x + epsilon
hist(y)
xlim<-c(min(x),max(x))
ylim<-c(min(y),max(y))
plot.window(xlim, ylim )
plot(x, y)
abline(v=mean(x))
abline(h=mean(y))
# two sample t test:
t.test(x,y,paired=FALSE)
# match pairs t test
t.test(x,y,paired=TRUE)
########################################
# Problem 8: Eigenfaces
library(bmp)
p1<-read.bmp("1_1.bmp")
p2<-read.bmp("5_1.bmp")
p3<-read.bmp("9_1.bmp")
# Rotate image by 90 degrees
rotate <- function(x) t(apply(x, 2, rev))
par(mfrow=c(1,3))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
# create a matrix of all 3 faces
faces<-matrix(c(as.vector(p1), as.vector(p2), as.vector(p3)), nrow = 3, byrow=TRUE)
# Caculate mean face and print all 4 faces
mean_face<-(p1+p2+p3)/3
par(mfrow=c(1,4))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
image(rotate(mean_face), col=gray((0:32)/32),axes=F)
mean2<-apply(faces, 2, mean)
par(mfrow=c(1,5))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
image(rotate(mean_face), col=gray((0:32)/32),axes=F)
image(rotate(mean2), col=gray((0:32)/32),axes=F)
mean2
image(rotate(as.matrix(mean2)), col=gray((0:32)/32),axes=F)
# create a matrix of all 3 faces
faces<-matrix(c(as.vector(p1), as.vector(p2), as.vector(p3)), nrow = 3, byrow=FALSE)
mean2<-apply(faces, 2, mean)
par(mfrow=c(1,5))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
image(rotate(mean_face), col=gray((0:32)/32),axes=F)
image(rotate(as.matrix(mean2)), col=gray((0:32)/32),axes=F)
par(mfrow=c(1,4))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
image(rotate(mean_face), col=gray((0:32)/32),axes=F)
# Differences of faces
p1diff<-p1-mean_face
p2diff<-p2-mean_face
p3diff<-p3-mean_face
par(mfrow=c(1,3))
image(rotate(p1diff), col=gray((0:32)/32),axes=F)
image(rotate(p2diff), col=gray((0:32)/32),axes=F)
image(rotate(p3diff), col=gray((0:32)/32),axes=F)
# create a matrix of differences for the faces
# calculate the covariance matrix and get the eigenvectors
faces_diff<-matrix(c(as.vector(p1diff), as.vector(p2diff), as.vector(p3diff)), nrow = 3, byrow=TRUE)
cov_faces_diff<-t(faces_diff)%*%faces_diff
eigenfaces<-eigen(cov_faces_diff)
# print the first 3 eigen vectors as images:
# Get the face weights:
fweight1<-as.vector(p1)%*%eigenfaces$vectors[,1]
fweight2<-as.vector(p2)%*%eigenfaces$vectors[,2]
fweight3<-as.vector(p3)%*%eigenfaces$vectors[,3]
reconface1<-as.matrix(fweight1*eigenfaces$vectors[,1] + mean_face, nrows=55, ncol(51))
reconface3<-as.matrix(fweight1*eigenfaces$vectors[,3] + mean_face, nrows=55, ncol(51))
reconface2<-as.matrix(fweight1*eigenfaces$vectors[,2] + mean_face, nrows=55, ncol(51))
par(mfrow=c(1,3))
image(rotate(reconface1), col=gray((0:32)/32),axes=F)
image(rotate(reconface3), col=gray((0:32)/32),axes=F)
image(rotate(reconface2), col=gray((0:32)/32),axes=F)
par(mfrow=c(1,3))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
par(mfrow=c(1,4))
image(rotate(p1), col=gray((0:32)/32),axes=F)
image(rotate(p2), col=gray((0:32)/32),axes=F)
image(rotate(p3), col=gray((0:32)/32),axes=F)
image(rotate(mean_face), col=gray((0:32)/32),axes=F)
par(mfrow=c(1,3))
image(rotate(p1diff), col=gray((0:32)/32),axes=F)
image(rotate(p2diff), col=gray((0:32)/32),axes=F)
image(rotate(p3diff), col=gray((0:32)/32),axes=F)
image(rotate(reconface1), col=gray((0:32)/32),axes=F)
image(rotate(reconface2), col=gray((0:32)/32),axes=F)
image(rotate(reconface3), col=gray((0:32)/32),axes=F)
func4<-function(t) (1+2.5*t)*exp(-0.3*t)
t<-seq(-3,30,0.1); y<-func4(t)
plot(t,y,type="l")
# Prepare the derivative functions for Newton-Raphson
dt<-deriv((~ (1+2.5*t)*exp(-0.3*t)), c("t"), func=TRUE, hessian=TRUE)
grad<-function(t)attr(dt(t), "gradient")[1]
hess<-function(t)attr(dt(t), "hessian")[1]
# Newton-Raphson:
t = 0.0
terminate = 10^-5
i<-0 # iteration counter
while(abs(grad(t))>terminate) {
t = t-grad(t)/hess(t)
i<-i+1
}
i # number of iterations
grad(t) # gradient
t; func4(t) # t and max value
text(t, func4(t), substitute("o"))
abline(v(t))
abline(v=t)
func4<-function(t) (1+2.5*t)*exp(-0.3*t)
t<-seq(-3,30,0.1); y<-func4(t)
plot(t,y,type="l")
dev.off()
func4<-function(t) (1+2.5*t)*exp(-0.3*t)
t<-seq(-3,30,0.1); y<-func4(t)
plot(t,y,type="l")
t = 0.0
terminate = 10^-5
i<-0 # iteration counter
while(abs(grad(t))>terminate) {
t = t-grad(t)/hess(t)
i<-i+1
}
i # number of iterations
grad(t) # gradient
t; func4(t) # t and max value
text(t, func4(t), substitute("o"))
abline(v=t)
A<-matrix(c(2,4,2,-2,0,1,0,0,0,3,3,-1,0,2,0,4), nrow=4, byrow=TRUE)
A_eig<-eigen(A)
A_eig$vectors
# Now use SVD to find the decomposition
A_svd<-svd(A)
U<-A_svd$u
V<-A_svd$v
eig<-A_eig$vectors
U
V
solve(U)
solve(V)
V%*%solve(V)
V
t(V)
solve(V)
U
d<-A_svd$d
d
d/sqrt(d)
d*sqrt(d)
9/17
2.22/9.5
3/4
4.49/6.68
a<-U[,1]
a
sqrt(t(a)%*%a)
sqrt(t(a)%*%a)0.1162/0.7730
0.1162/0.7730
U-eig
U/eig
eig
U
solve(eig)%*%A%*%eig
U%*%diag(d)%*%V
?svd
U
V
U%*%diag(d)
U%*%diag(d)%*%solve(U)
U%*%diag(A_eig$values)%*%solve(U)
V%*%t(diag(d))%*%diag(d)%*%t(V)
solve(A)%*%V%*%t(diag(d))%*%diag(d)%*%t(V)
U%*%diag(A_eig$values)%*%solve(U)
V%*%diag(d)%*%t(V)
U%*%diag(d)%*%V
M <- as.matrix(c(2, -3, -5, 1, 1, 2, 2, 0), byrow=TRUE, nrow=4)
M
M <- as.matrix(c(2, -3, -5, 1, 1, 2, 2, 0), nrow = 4, byrow=TRUE)
M <- matrix(c(2, -3, -5, 1, 1, 2, 2, 0), nrow = 4, byrow=TRUE)
M
svd(M)
SM<-svd(M)
SM$u%*%diag(SM$d)%*%SM$v
U%*%sqrt(diag(d))%*%V
# SVD of A
U%*%diag(d)%*%V
B_inv%*%A%*%B
B<-A_eig$vectors
B_inv<-solve(B)
B_inv%*%A%*%B
# SVD of A
U%*%diag(d)%*%V
# This does not return the matrix A...
install.packages(c("cluster", "deSolve", "foreign", "MASS", "Matrix", "mgcv", "nlme", "rpart", "survival"))
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
MAKEVARS <- file.path(dotR, "Makevars")
if (!file.exists(MAKEVARS)) file.create(MAKEVARS)
cat(
"\nCXXFLAGS=-O3 -mtune=native -march=native -Wno-unused-variable -Wno-unused-function  -Wno-macro-redefined -Wno-unknown-pragmas",
"\nCC=clang",
"CXX=clang++ -arch x86_64 -ftemplate-depth-256",
file = MAKEVARS,
sep = "\n",
append = TRUE
)
# print the contents to the console
cat(readLines(MAKEVARS), sep = "\n")
cat(readLines(MAKEVARS), sep = "\n")
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)
fx <- inline::cxxfunction( signature(x = "integer", y = "numeric" ) , '
return ScalarReal( INTEGER(x)[0] * REAL(y)[0] ) ;
' )
fx( 2L, 5 )
data {
int<lower=0> J;          // number of schools
real y[J];               // estimated treatment effects
real<lower=0> sigma[J];  // s.e. of effect estimates
}
parameters {
real mu;
real<lower=0> tau;
vector[J] eta;
}
transformed parameters {
vector[J] theta;
theta = mu + tau * eta;
}
model {
target += normal_lpdf(eta | 0, 1);
target += normal_lpdf(y | theta, sigma);
}
schools_data <- list(
J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18)
)
schools_data
library(rstan)
fit1 <- stan(
file = "schools.stan",  # Stan program
data = schools_data,    # named list of data
chains = 4,             # number of Markov chains
warmup = 1000,          # number of warmup iterations per chain
iter = 2000,            # total number of iterations per chain
cores = 2,              # number of cores (using 2 just for the vignette)
refresh = 1000          # show progress every 'refresh' iterations
)
pwd
path
library(rstan)
fit1 <- stan(
file = "schools.stan",  # Stan program
data = schools_data,    # named list of data
chains = 4,             # number of Markov chains
warmup = 1000,          # number of warmup iterations per chain
iter = 2000,            # total number of iterations per chain
cores = 2,              # number of cores (using 2 just for the vignette)
refresh = 1000          # show progress every 'refresh' iterations
)
print(fit1, pars=c("theta", "mu", "tau", "lp__"), probs=c(.1,.5,.9))
install.packages("drat")
drat::addRepo("jr-packages")
install.packages("jrRstan")
#comment
mle = 7/20
curve(dbeta(x,1,1),from=0,to=1))
curve(dbeta(x,1,1),from=0,to=1)
curve(dbeta(x,8,14),from=0,to=1)
qbeta(c(0.025,0.975),8,14)
curve(dbeta(x,1.4,5.6),from=0,to=1)
curve(dbeta(x,8.4,18.6),from=0,to=1)
curve(dbeta(x,1.4,5.6),from=0,to=1)
curve(dbeta(x,8.4,18.6),from=0,to=1, add=TRUE)
curve(dbeta(x,1.4,5.6),from=0,to=1, ylim=c(0,7))
curve(dbeta(x,8.4,18.6),from=0,to=1, add=TRUE)
qbeta(c(0.025,0.975),8.4,18.6)
curve(dbeta(x,1.4,5.6),from=0,to=1, ylim=c(0,7))
curve(dbeta(x,6.4,15.6),from=0,to=1, add=TRUE)
qbeta(c(0.025,0.975),6.4,15.6)
qbeta(c(0.025,0.975),10.4,41.6)
curve(dbeta(x,10.4,41.6),from=0,to=1, add=TRUE)
n = 100
sim.post.100<-rbeta(n,8,14)
hist(sim.post.100,freq=F)
sim.post.100<-rbeta(n,8,14)
hist(sim.post.100,freq=F)
hist(sim.post.100,prob=T
)
hist(sim.post.100,prob=T)
curve(dbeta(x,8,14),add=T)
# for a uniform p(theta)
a_theta = 1
hist(sim.post.100,freq=F)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,8,14)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.100,quant)
cred.theory<-qbeta(quant,a,b)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.100,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
log.odds.100<-log(sim.post.100/(1-sim.post.100))
hist(log.odds.100)
quantile(log.odds.100,quant)
# Q2 10000 samples
n = 20 # no of independent trials
y = 7 # number of successes
ns = 10000 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.100,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
# plot the log odds:
log.odds.100<-log(sim.post.100/(1-sim.post.100))
hist(log.odds.100)
quantile(log.odds.100,quant)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.100,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
# plot the log odds:
log.odds.100<-log(sim.post.100/(1-sim.post.100))
hist(log.odds.100)
quantile(log.odds.100,quant)
# Q2 10000 samples
n = 20 # no of independent trials
y = 7 # number of successes
ns = 10000 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.10000<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.10000,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.10000,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
# plot the log odds:
log.odds.100<-log(sim.post.10000/(1-sim.post.10000))
hist(log.odds.10000)
quantile(log.odds.10000,quant)
# Q2
n = 20 # no of independent trials
y = 7 # number of successes
ns = 100 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.100<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.100,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.100<-quantile(sim.post.100,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
# plot the log odds:
log.odds.100<-log(sim.post.100/(1-sim.post.100))
hist(log.odds.100)
quantile(log.odds.100,quant)
# Q2 10000 samples
n = 20 # no of independent trials
y = 7 # number of successes
ns = 10000 # no of samples
# for a uniform p(theta)
a_theta = 1
b_theta = 1
a = a_theta+y
b = b_theta+n-y
sim.post.10000<-rbeta(ns,a,b)  # generate 100 simulations
hist(sim.post.10000,freq=F)
curve(dbeta(x,a,b),add=T)
# estimate 95% conf
quant<-c(0.025,0.975)
cred.10000<-quantile(sim.post.10000,quant)  # 95% interval of sampled data
cred.theory<-qbeta(quant,a,b)  # 95% interval for theoretical beta model.
# plot the log odds:
log.odds.10000<-log(sim.post.10000/(1-sim.post.10000))
hist(log.odds.10000)
quantile(log.odds.10000,quant)
?dunif
dunif(0.2,0,1)
rnorm(1,0,1)
y = rnorm(10000,0,1)
y
mean(y)
hist(y)
sd(y)
setwd("/Users/perusha/git_repos/UoR-Bayesian-Course/")
y = rnorm(100000, 0, 1)
hist(y)
mean(y)
sd(y)
setwd("/Users/perusha/git_repos/UoR-Bayesian-Course/")
z = y^2
hist(z)
